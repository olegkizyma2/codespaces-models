# üîç –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ OpenAI API –°—É–º—ñ—Å–Ω–æ—Å—Ç—ñ

## ‚úÖ –°—Ç–∞—Ç—É—Å: –ü–û–í–ù–ê –°–£–ú–Ü–°–ù–Ü–°–¢–¨

–ù–∞—à –ø—Ä–æ–∫—Å—ñ-—Å–µ—Ä–≤–µ—Ä –Ω–∞ `http://localhost:4000` **–ø–æ–≤–Ω—ñ—Å—Ç—é —Å—É–º—ñ—Å–Ω–∏–π** –∑ OpenAI SDK —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º—É—î –≤—Å—ñ –æ—Å–Ω–æ–≤–Ω—ñ endpoints.

---

## üìã –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ Endpoints

### üî• –û—Å–Ω–æ–≤–Ω—ñ OpenAI Endpoints

#### 1Ô∏è‚É£ **GET /v1/models**
```bash
curl http://localhost:4000/v1/models
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –ü–æ–≤–µ—Ä—Ç–∞—î 58 –º–æ–¥–µ–ª–µ–π
- üìä **–§–æ—Ä–º–∞—Ç**: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π OpenAI format
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å–ø–∏—Å–∫—É –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

**–ü—Ä–∏–∫–ª–∞–¥ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ:**
```json
{
  "data": [
    {"id": "gpt-4o-mini", "object": "model", ...},
    {"id": "microsoft/phi-3.5-mini-instruct", ...},
    ...58 models total
  ]
}
```

---

#### 2Ô∏è‚É£ **POST /v1/chat/completions**
```bash
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
  }'
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –ü–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–∏–π chat completions
- üåä **Streaming**: –ü—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è (`stream: true`)
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –û—Å–Ω–æ–≤–Ω–∏–π endpoint –¥–ª—è —á–∞—Ç—ñ–≤

**–£—Å–ø—ñ—à–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:**
```json
{
  "choices": [{
    "message": {
      "role": "assistant",
      "content": "Hi! How can I assist you today?"
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 10,
    "total_tokens": 19
  }
}
```

**–ó streaming:**
```bash
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": true
  }'
```
–ü–æ–≤–µ—Ä—Ç–∞—î SSE (Server-Sent Events):
```
data: {"choices":[{"delta":{"content":"Hi"}}]}
data: {"choices":[{"delta":{"content":"!"}}]}
data: [DONE]
```

---

#### 3Ô∏è‚É£ **POST /v1/completions** (Legacy)
```bash
curl -X POST http://localhost:4000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "prompt": "Hello, my name is",
    "stream": false,
    "max_tokens": 10
  }'
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –ö–æ–Ω–≤–µ—Ä—Ç—É—î prompt ‚Üí chat messages
- üåä **Streaming**: –ü—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: Legacy text completion

**–í–Ω—É—Ç—Ä—ñ—à–Ω—è –ª–æ–≥—ñ–∫–∞:**
```javascript
// Prompt –∫–æ–Ω–≤–µ—Ä—Ç—É—î—Ç—å—Å—è –≤ messages
prompt: "Hello" ‚Üí messages: [{"role": "user", "content": "Hello"}]
// –ü–æ—Ç—ñ–º –≤–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è chat.completions
```

---

### üÜï –î–æ–¥–∞—Ç–∫–æ–≤—ñ Endpoints (–†–æ–∑—à–∏—Ä–µ–Ω–Ω—è)

#### 4Ô∏è‚É£ **GET /v1/tokens/stats**
```bash
curl http://localhost:4000/v1/tokens/stats
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –°—Ç–∞—Ç—É—Å –≤—Å—ñ—Ö —Ç–æ–∫–µ–Ω—ñ–≤
- üîÑ **–û–Ω–æ–≤–ª–µ–Ω–Ω—è**: Real-time
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ä–æ—Ç–∞—Ü—ñ—ó —Ç–æ–∫–µ–Ω—ñ–≤

**–í—ñ–¥–ø–æ–≤—ñ–¥—å:**
```json
{
  "current_token": "GITHUB_TOKEN3",
  "total_tokens": 4,
  "tokens": [
    {
      "key": "GITHUB_TOKEN",
      "blocked": false,
      "failures": 0,
      "lastUsed": null
    },
    ...
  ]
}
```

---

#### 5Ô∏è‚É£ **POST /v1/tokens/rotate**
```bash
curl -X POST http://localhost:4000/v1/tokens/rotate
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –†—É—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–≤
- üîÑ **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –ü–µ—Ä–µ–∫–ª—é—á–∞—î –Ω–∞ –Ω–∞—Å—Ç—É–ø–Ω–∏–π —Ç–æ–∫–µ–Ω
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –û–±—Ö—ñ–¥ rate limits

**–í—ñ–¥–ø–æ–≤—ñ–¥—å:**
```json
{
  "success": true,
  "message": "–¢–æ–∫–µ–Ω —É—Å–ø—ñ—à–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–æ"
}
```

---

#### 6Ô∏è‚É£ **POST /v1/tokens/reset-stats**
```bash
curl -X POST http://localhost:4000/v1/tokens/reset-stats
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –°–∫–∏–¥–∞—î —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–∞ circuit breakers
- üîÑ **–†–µ–∑—É–ª—å—Ç–∞—Ç**: –í—ñ–¥–Ω–æ–≤–ª—é—î –≤—Å—ñ —Ç–æ–∫–µ–Ω–∏
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –ü—ñ—Å–ª—è –≤–∏—Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ–±–ª–µ–º

---

#### 7Ô∏è‚É£ **GET /v1/rate-limits/observed**
```bash
curl http://localhost:4000/v1/rate-limits/observed
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –°–ø–æ—Å—Ç–µ—Ä–µ–∂—É–≤–∞–Ω—ñ rate limits
- üìä **–î–∞–Ω—ñ**: RPM, TPM –¥–ª—è –∫–æ–∂–Ω–æ—ó –º–æ–¥–µ–ª—ñ
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: –ê–Ω–∞–ª—ñ–∑ –æ–±–º–µ–∂–µ–Ω—å

---

#### 8Ô∏è‚É£ **POST /v1/embeddings**
```bash
curl -X POST http://localhost:4000/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "text-embedding-3-small",
    "input": "Hello world"
  }'
```
- ‚úÖ **–ü—Ä–∞—Ü—é—î**: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è embeddings
- üéØ **–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è**: Vector search, RAG

---

## üåê –í–µ–±-—ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ü–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è

### –ù–∞—à –≤–µ–± –Ω–∞ `http://localhost:4000` –∑–≤–µ—Ä—Ç–∞—î—Ç—å—Å—è –¥–æ:

```javascript
// 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π
fetch('http://localhost:4000/v1/models')

// 2. Chat —Ä–µ–∂–∏–º
fetch('http://localhost:4000/v1/chat/completions', {
  method: 'POST',
  body: JSON.stringify({
    model: selectedModel,
    messages: conversationHistory,
    stream: true
  })
})

// 3. Completion —Ä–µ–∂–∏–º
fetch('http://localhost:4000/v1/completions', {
  method: 'POST',
  body: JSON.stringify({
    model: selectedModel,
    prompt: userInput,
    stream: true
  })
})

// 4. Token stats
fetch('http://localhost:4000/v1/tokens/stats')
```

**–í—Å–µ –ø—Ä–∞—Ü—é—î –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å—ñ!** ‚úÖ

---

## üîß OpenAI SDK –°—É–º—ñ—Å–Ω—ñ—Å—Ç—å

### Python SDK
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:4000/v1",
    api_key="dummy"  # –ù–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω, –∞–ª–µ SDK –≤–∏–º–∞–≥–∞—î
)

# Chat
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello"}]
)

# Completions
response = client.completions.create(
    model="gpt-4o-mini",
    prompt="Hello"
)
```

‚úÖ **–ü—Ä–∞—Ü—é—î –ø–æ–≤–Ω—ñ—Å—Ç—é!**

---

### Node.js SDK
```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  baseURL: 'http://localhost:4000/v1',
  apiKey: 'dummy'
});

// Chat
const completion = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello' }],
});

// Stream
const stream = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello' }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

‚úÖ **–ü—Ä–∞—Ü—é—î –ø–æ–≤–Ω—ñ—Å—Ç—é!**

---

## üéØ –§–æ—Ä–º–∞—Ç –í—ñ–¥–ø–æ–≤—ñ–¥–µ–π

### Chat Completions (Non-streaming)
```json
{
  "id": "chatcmpl-...",
  "object": "chat.completion",
  "created": 1760094170,
  "model": "gpt-4o-mini-2024-07-18",
  "choices": [{
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "Response text",
      "refusal": null
    },
    "finish_reason": "stop"
  }],
  "usage": {
    "prompt_tokens": 9,
    "completion_tokens": 10,
    "total_tokens": 19
  }
}
```

### Chat Completions (Streaming)
```
data: {"id":"chatcmpl-...","choices":[{"delta":{"role":"assistant"}}]}
data: {"id":"chatcmpl-...","choices":[{"delta":{"content":"Hi"}}]}
data: {"id":"chatcmpl-...","choices":[{"delta":{"content":"!"}}]}
data: {"id":"chatcmpl-...","choices":[{"finish_reason":"stop"}]}
data: [DONE]
```

### Completions (Non-streaming)
```json
{
  "id": "cmpl-...",
  "object": "text_completion",
  "created": 1760094170,
  "model": "gpt-4o-mini",
  "choices": [{
    "text": "Response text",
    "index": 0,
    "finish_reason": "stop"
  }]
}
```

---

## ‚ö†Ô∏è –ü–æ—Ç–æ—á–Ω—ñ –ü—Ä–æ–±–ª–µ–º–∏

### 1. Rate Limiting (429 Errors)
```json
{
  "error": {
    "message": "Upstream rate limit reached (UserByModelByDay). Retry after ~20663s.",
    "type": "rate_limit_exceeded",
    "param": "model",
    "code": "rate_limit"
  }
}
```

**–†—ñ—à–µ–Ω–Ω—è:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–≤ (4 —Ç–æ–∫–µ–Ω–∏)
- ‚úÖ Throttling (2000ms –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏)
- ‚úÖ Circuit breaker –¥–ª—è –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
- üîß –†—É—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è: `curl -X POST http://localhost:4000/v1/tokens/rotate`

### 2. Circuit Breaker
```json
{
  "error": {
    "message": "Circuit breaker open for model: gpt-4o-mini",
    "type": "invalid_request_error"
  }
}
```

**–†—ñ—à–µ–Ω–Ω—è:**
- –°–∫–∏–Ω—É—Ç–∏: `curl -X POST http://localhost:4000/v1/tokens/reset-stats`
- –ê–±–æ –ø–æ—á–µ–∫–∞—Ç–∏ ~1 —Ö–≤–∏–ª–∏–Ω—É (auto-recovery)

---

## üìä –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è

### –®–≤–∏–¥–∫–∏–π —Ç–µ—Å—Ç –≤—Å—ñ—Ö endpoints:
```bash
# 1. Models
curl http://localhost:4000/v1/models | jq '.data | length'
# Expected: 58

# 2. Chat
curl -X POST http://localhost:4000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"gpt-4o-mini","messages":[{"role":"user","content":"Hi"}],"stream":false,"max_tokens":5}' \
  | jq '.choices[0].message.content'
# Expected: "Hi! How can..."

# 3. Token stats
curl http://localhost:4000/v1/tokens/stats | jq '.total_tokens'
# Expected: 4

# 4. Rotate token
curl -X POST http://localhost:4000/v1/tokens/rotate | jq '.success'
# Expected: true
```

---

## ‚úÖ –í–∏—Å–Ω–æ–≤–æ–∫

### –ù–∞—à –ø—Ä–æ–∫—Å—ñ **–ü–û–í–ù–Ü–°–¢–Æ –°–£–ú–Ü–°–ù–ò–ô** –∑ OpenAI API! üéâ

- ‚úÖ –í—Å—ñ –æ—Å–Ω–æ–≤–Ω—ñ endpoints –ø—Ä–∞—Ü—é—é—Ç—å
- ‚úÖ Streaming –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è
- ‚úÖ Python SDK —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å
- ‚úÖ Node.js SDK —Å—É–º—ñ—Å–Ω—ñ—Å—Ç—å
- ‚úÖ –í–µ–±-—ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å —ñ–Ω—Ç–µ–≥—Ä–æ–≤–∞–Ω–∏–π
- ‚úÖ –î–æ–¥–∞—Ç–∫–æ–≤—ñ endpoints –¥–ª—è –º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥—É
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∞ —Ä–æ—Ç–∞—Ü—ñ—è —Ç–æ–∫–µ–Ω—ñ–≤
- ‚úÖ Rate limit handling

### –ú–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —è–∫ drop-in replacement –¥–ª—è OpenAI API!

```python
# –ó–∞–º—ñ—Å—Ç—å
client = OpenAI()

# –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ
client = OpenAI(base_url="http://localhost:4000/v1", api_key="dummy")
```

**–í—Å–µ –ø—Ä–∞—Ü—é—î! üöÄ**

---

**–î–∞—Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏**: 2025-10-10  
**–í–µ—Ä—Å—ñ—è API**: OpenAI v1  
**–°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–µ—Ä–∞**: Online –Ω–∞ http://localhost:4000
